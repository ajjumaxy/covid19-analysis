{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim as gensim\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and retrieve tweets from Singapore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read twitter data\n",
    "\n",
    "tweets_data_path = 'coronavirus_tweets_20200203.txt'\n",
    "\n",
    "def get_tweetlist(filepath):\n",
    "    \n",
    "    tweets_file = open(filepath, 'r')\n",
    "    tweets_list = []\n",
    "\n",
    "    for line in tweets_file:\n",
    "        if len(line.strip()) > 0:\n",
    "            tweet = json.loads(line) # each line is a JSON object\n",
    "            if 'text' in tweet:\n",
    "                tweets_list.append(tweet)\n",
    "    \n",
    "    return tweets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list = get_tweetlist(tweets_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample tweet\n",
    "# tweets_list[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and retrieve tweets from Singapore, and return selected fields in a dataframe\n",
    "\n",
    "def get_sgtweet_df(tweetlist):\n",
    "    \n",
    "    tweets_df = pd.DataFrame()\n",
    "\n",
    "    tweets_df['text'] = list(map(lambda tweet: tweet['text'], tweets_list))\n",
    "\n",
    "    tweets_df['lang'] = list(map(lambda tweet: tweet['lang'], tweets_list))\n",
    "\n",
    "    tweets_df['country_code'] = list(map(lambda tweet: tweet['place']['country_code']\n",
    "                            if tweet['place'] != None\n",
    "                            else\n",
    "                                None,\n",
    "                        tweets_list))\n",
    "\n",
    "    tweets_df['location'] = list(map(lambda tweet: tweet['user']['location']\n",
    "                            if tweet['user'] != None\n",
    "                            else\n",
    "                                None,\n",
    "                        tweets_list))\n",
    "\n",
    "    tweets_df['timestamp'] = list(map(lambda tweet: tweet['timestamp_ms'], tweets_list))\n",
    "\n",
    "    fulltext_list = []\n",
    "    for tweet in tweets_list:\n",
    "        try:\n",
    "            fulltext_list.append(tweet['extended_tweet']['full_text'])\n",
    "        except KeyError:\n",
    "            fulltext_list.append(None)\n",
    "    tweets_df['full text'] = fulltext_list \n",
    "    \n",
    "    \n",
    "    #get SG tweets only\n",
    "    tweets_sg = tweets_df.loc[ (tweets_df['location'].str.contains('Singapore', na=False)) | (tweets_df['country_code'] == 'SG') ].copy()\n",
    "    tweets_sg['timestamp'] = pd.to_datetime(tweets_sg['timestamp'], unit='ms')\n",
    "    tweets_sg.head()\n",
    "    \n",
    "    return tweets_sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>country_code</th>\n",
       "      <th>location</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>full text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>RT @MoneyTalkR3: Despite the liquidity support...</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>2020-02-03 02:37:16.400</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>RT @haloefekti: #CoronavirusOutbreak Russians ...</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>2020-02-03 02:38:47.464</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>RT @Indounik: While #coronavirus is on the min...</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>Outram Road, Singapore</td>\n",
       "      <td>2020-02-03 02:41:05.296</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>They must be law enforcement personnel. This i...</td>\n",
       "      <td>en</td>\n",
       "      <td>SG</td>\n",
       "      <td>India</td>\n",
       "      <td>2020-02-03 02:41:15.373</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>Hopefully this one succeeds. üôèüèª#coronavirus\\n#...</td>\n",
       "      <td>en</td>\n",
       "      <td>SG</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>2020-02-03 02:42:14.787</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text lang country_code  \\\n",
       "152   RT @MoneyTalkR3: Despite the liquidity support...   en         None   \n",
       "672   RT @haloefekti: #CoronavirusOutbreak Russians ...   en         None   \n",
       "1519  RT @Indounik: While #coronavirus is on the min...   en         None   \n",
       "1582  They must be law enforcement personnel. This i...   en           SG   \n",
       "1894  Hopefully this one succeeds. üôèüèª#coronavirus\\n#...   en           SG   \n",
       "\n",
       "                    location               timestamp full text  \n",
       "152                Singapore 2020-02-03 02:37:16.400      None  \n",
       "672                Singapore 2020-02-03 02:38:47.464      None  \n",
       "1519  Outram Road, Singapore 2020-02-03 02:41:05.296      None  \n",
       "1582                   India 2020-02-03 02:41:15.373      None  \n",
       "1894               Singapore 2020-02-03 02:42:14.787      None  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgtweets_df = get_sgtweet_df(tweets_list)\n",
    "sgtweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns Singapore tweets (text field) as a list, for text preprocessing and analysis\n",
    "\n",
    "def get_sgtweet_list(tweetdf):\n",
    "    \n",
    "    text_list = tweetdf['text'].tolist()\n",
    "    fulltext_list = tweetdf['full text'].tolist()\n",
    "    num_tweets = len(text_list)\n",
    "\n",
    "    sgtweets_list = []\n",
    "    for i in range(0, num_tweets):\n",
    "        if fulltext_list[i] == None:\n",
    "            sgtweets_list.append(text_list[i])\n",
    "        else:\n",
    "            sgtweets_list.append(fulltext_list[i])\n",
    "            \n",
    "    return sgtweets_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @MoneyTalkR3: Despite the liquidity support from the #PBOC and reverse repo cuts in #China the yuan has weakened below 7 versus the USD.‚Ä¶',\n",
       " 'RT @haloefekti: #CoronavirusOutbreak Russians evacuated from China https://t.co/ubRZ2fmOIX',\n",
       " 'RT @Indounik: While #coronavirus is on the minds of many in Bali where there have been no confirmed cases, Balinese animal health authoriti‚Ä¶',\n",
       " \"They must be law enforcement personnel. This is what's fake news &amp; Rumor mongering.\",\n",
       " 'Hopefully this one succeeds. üôèüèª#coronavirus\\n#CoronavirusOutbreak \\n\\nhttps://t.co/Hu4waCjEEK']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_tweets = get_sgtweet_list(sgtweets_df)\n",
    "sg_tweets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#splits string into substrings using a regular expression\n",
    "#tokenises words that contain 1 or more (+) alphanumeric characters (\\w) or '@'\n",
    "retokenizer = RegexpTokenizer(r'[\\w@]+')\n",
    "\n",
    "# create stop words list\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# add twitter specific stop words\n",
    "stop_words = stop_words + [\"rt\", \"got\", \"thats\", \"would\", \"going\", \"u\", \"get\", \"also\", \"one\", \"could\", \"said\", \"like\", \"via\"]\n",
    "\n",
    "# create lemmatizer\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def remove_urls(s):\n",
    "    # re.sub(pattern,repl,string) is used to replace substrings. Will replace the matches in string with repl\n",
    "    return re.sub(r'https?://\\S+', \"\", s)\n",
    "\n",
    "def remove_usernames(s):\n",
    "    return re.sub(r'@\\S+', \"\", s)\n",
    "\n",
    "def remove_specialchar(text, remove_digits=False):    \n",
    "    if not remove_digits:\n",
    "        pattern = r'[^a-zA-z0-9\\s]'\n",
    "    else:\n",
    "        pattern = r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(tweetlist):\n",
    "    processed_docs = []\n",
    "    for doc in tweetlist:\n",
    "        doc = doc.lower()\n",
    "        doc = remove_urls(doc)\n",
    "        doc = remove_usernames(doc)\n",
    "        doc = remove_specialchar(doc, remove_digits=False)\n",
    "        doc_tokens = retokenizer.tokenize(doc)\n",
    "        doc_tokens = [token for token in doc_tokens if token not in stop_words]\n",
    "        doc = ' '.join(doc_tokens)\n",
    "        processed_docs.append(doc)\n",
    "        \n",
    "    return processed_docs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['despite liquidity support pboc reverse repo cuts china yuan weakened 7 versus usd',\n",
       " 'coronavirusoutbreak russians evacuated china',\n",
       " 'coronavirus minds many bali confirmed cases balinese animal health authoriti',\n",
       " 'must law enforcement personnel whats fake news amp rumor mongering',\n",
       " 'hopefully succeeds coronavirus coronavirusoutbreak',\n",
       " 'coronavirus train italy teenage chinese boy boards train woman comments loudly go',\n",
       " 'chinas futures market slumped open mon iron ore screw thread crude oil palm oil eggs contracts fa',\n",
       " 'novel coronavirus likely much wider deeper impact chinas economy trade singapore rest',\n",
       " 'wrong real coronavirus number went model yesterday im shocked extended vacation immediately saw green prediction curve wise inflection point coming soon take care everyone',\n",
       " 'coronavirus concerns may cause grab taxi late days warned coronavirus grab',\n",
       " 'novel coronavirus likely much wider deeper impact chinas economy trade singapore rest',\n",
       " 'thai doctors say key treating coronavirus combination flu hiv drugs coronavirus thai china coronavirusoutb',\n",
       " 'health officials confirmed eighth case flulike coronavirus united states latest patient mas',\n",
       " '30000 chinese work pass holders yet return china josephine teo coronavirus letstalkcity business workpass china singapore southeastasia',\n",
       " '30000 chinese work pass holders yet return china josephine teo coronavirus letstalkcity business workpass china singapore southeastasia',\n",
       " 'talking friend works pharmacy masks stock coronavirus sold hour mult',\n",
       " 'know errant retailer profiteering selling surgical masks premium singapore please report case consumer authority 61000315 please share coronavirus',\n",
       " 'india baarat ka swagat pan parag se hi hona chahiye meanwhile indonesia',\n",
       " 'coronavirus train italy teenage chinese boy boards train woman comments loudly go',\n",
       " 'latest wuhanpneumonia china feb 2 17205 confirmed cases 361 deaths 475 discharged hospitals af',\n",
       " 'hahahhaha',\n",
       " 'jangan irritating please youre sick stay home coronavirus lepakonekorner',\n",
       " 'china long history bad press regarding ppl misbehaving abroad needs step educate ppl acceptable behaviours overnight campaign want fix image issues look singapore campaigns past coronavirus']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tweets = process_text(sg_tweets)\n",
    "processed_tweets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
